<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><title>k-近邻算法实现数字识别 - 侯锐的思考与分享</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="keywords" content="机器学习,Python"><meta name="description" content="&lt;p&gt;&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&#34;&gt;机器学习&lt;/a&gt;（Machine Learning, ML）是人工智能（Artificial Intelligence, AI）的一个重要的分支，机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。K-NN（K Nearest Neighbor）算法是机器学习中最基础也是最简单的算法之一，下面我们就来了解一下这个算法是怎么工作以及是怎么帮助我们解决问题的。&lt;/p&gt;
&lt;h3 id=&#34;1-kNN算法的原理&#34;&gt;&lt;a href=&#34;#1-kNN算法的原理&#34; class=&#34;headerlink&#34; title=&#34;1. kNN算法的原理&#34;&gt;&lt;/a&gt;1. kNN算法的原理&lt;/h3&gt;&lt;p&gt;kNN算法的原理极其简单，即首先存在一个训练样本数据集，且训练集中的每一个数据都会存在一些&lt;em&gt;特征&lt;/em&gt;，并且该数据会拥有一个已知的&lt;em&gt;分类&lt;/em&gt;。在输入未知分类的测试数据的时候，根据它的特征内容将测试数据和每一条训练数"><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="侯锐的思考与分享" type="application/atom+xml"></head><body><div class="container"><header class="header"><div class="blog-title"><a href="/" class="logo">侯锐的思考与分享</a><div class="subtitle"></div></div><nav class="navbar"><ul class="menu"><li class="menu-item"><a href="/" class="menu-item-link" data-no-instant>主页</a></li><li class="menu-item"><a href="/atom.xml" class="menu-item-link" data-no-instant>订阅</a></li><li class="menu-item"><a href="/search" class="menu-item-link" data-no-instant>搜索</a></li></ul></nav></header><article class="post"><div class="post-title"><h1 class="article-title">k-近邻算法实现数字识别</h1></div><div class="post-meta"><span class="post-time">2017-05-26</span></div><div class="post-content"><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>（Machine Learning, ML）是人工智能（Artificial Intelligence, AI）的一个重要的分支，机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。K-NN（K Nearest Neighbor）算法是机器学习中最基础也是最简单的算法之一，下面我们就来了解一下这个算法是怎么工作以及是怎么帮助我们解决问题的。</p><h3 id="1-kNN算法的原理"><a href="#1-kNN算法的原理" class="headerlink" title="1. kNN算法的原理"></a>1. kNN算法的原理</h3><p>kNN算法的原理极其简单，即首先存在一个训练样本数据集，且训练集中的每一个数据都会存在一些<em>特征</em>，并且该数据会拥有一个已知的<em>分类</em>。在输入未知分类的测试数据的时候，根据它的特征内容将测试数据和每一条训练数据进行比较，然后提取出训练数据中与测试数据最相似的k个数据（这就是kNN算法名称的来历），然后取出这个k个数据中出现次数最多的分类，作为测试数据的分类。</p><h3 id="2-如何通过特征来比较训练数据和测试数据的相似度"><a href="#2-如何通过特征来比较训练数据和测试数据的相似度" class="headerlink" title="2. 如何通过特征来比较训练数据和测试数据的相似度"></a>2. 如何通过特征来比较训练数据和测试数据的相似度</h3><p>通过特征值来计算相似度需要用到一点点的数学内容，如果训练数据称为A，测试数据为B，这个数据共有1, 2, …, n个特征，则它们的相似度可以使用如下公式表示：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>x</mi><mo>&#x3D;</mo><msqrt><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>i</mi><mo>&#x3D;</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo>−</mo><msub><mi>B</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></msqrt></math></p><p>方法很简单，就是取它们的特征之差的平方和，之后开平方得到x，x越小则说明这两个数据在当前特征下越相似。</p><p>但是，不同的数据之间可能存在数值上的差异。例如，对于一个上市公司，其营业额是必然大于其员工数的，那么此时把营业额的差值和员工数的差值同等对待显然是不合适的。利用下面的公式可以把任意取值范围的特征值转化到0到1区间之内的值：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi><mo>&#x3D;</mo><mfrac><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi><mo>−</mo><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi><mo>−</mo><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow></mfrac></mrow></math></p><p>其中min和max分别为数据集中的最小特征值和最大特征值。接下来，只需要使用newValue作为新的特征值就可以了，虽然这样增加了计算量，不过我们必须这样做。至此我们已经知道了kNN算法的基本工作方式，下面我们通过一个实际的例子来了解一下KNN算法是如何工作的。</p><h3 id="3-kNN算法实现数字识别"><a href="#3-kNN算法实现数字识别" class="headerlink" title="3. kNN算法实现数字识别"></a>3. kNN算法实现数字识别</h3><p>想要通过kNN算法实现数字识别，首先我们需要提供大量已知数字值的训练数据，之后把测试数据扔到任务中去与训练数据比对，最终分析出该数字的可能值。</p><p>这里存放了一些<a target="_blank" rel="noopener" href="https://github.com/RitterHou/learn_knn/tree/master/digits/trainingDigits">训练数据</a>，每一个文件都对应一个数字，并且数字的真实值在文件名中表示了出来。<a target="_blank" rel="noopener" href="https://github.com/RitterHou/learn_knn/tree/master/digits/testDigits">测试数据</a>也同样的是一个文件对应一个数字，我们的任务就是把测试数据文件中的数字值给识别出来。</p><p>下面我们使用Python来实现kNN算法（需要安装numpy模块），程序主要就分为这几步：</p><ol><li>读入训练数据构造为一个矩阵，把训练文件的字符串通过取出每一行然后首尾相连的方式构造为一个字符数组，每一个训练数组都作为矩阵的一行；</li><li>通过同样的方式把测试数据构造为一个只有一行的矩阵；</li><li>把测试数据同每一条训练数据做比较，选出差异最小的k条训练数据；</li><li>把k条数据中所占比例最大的数字作为测试数字的识别结果；</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> zeros, tile</span><br><span class="line"></span><br><span class="line">TRAINING_PATH = <span class="string">&#x27;digits/trainingDigits/&#x27;</span></span><br><span class="line">TRAINING_REAL_NUMS = []  <span class="comment"># 训练数据的真实数字</span></span><br><span class="line">TEST_PATH = <span class="string">&#x27;digits/testDigits/&#x27;</span></span><br><span class="line">TEST_REAL_NUMS = []  <span class="comment"># 测试数据的真实数字</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_matrix_by_file</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    把指定文件内的数据生成一个一维矩阵</span></span><br><span class="line"><span class="string">    :param file:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    file_data_array = zeros((<span class="number">1</span>, <span class="number">1024</span>))  <span class="comment"># 创建一个只有一行且存储文件所有数据的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file) <span class="keyword">as</span> lines:</span><br><span class="line">        <span class="comment"># 把文件的所有数据存储在矩阵中</span></span><br><span class="line">        <span class="keyword">for</span> line_num, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(lines):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">                file_data_array[<span class="number">0</span>, <span class="number">32</span> * line_num + i] = <span class="built_in">int</span>(line[i])</span><br><span class="line">    <span class="keyword">return</span> file_data_array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_training_matrix</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取所有训练文件的数据并且转化为一个矩阵</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    training_file_list = listdir(TRAINING_PATH)  <span class="comment"># 获取训练文件列表</span></span><br><span class="line">    training_matrix = zeros((<span class="built_in">len</span>(training_file_list), <span class="number">1024</span>))  <span class="comment"># 创建一个矩阵，行数为训练文件数，列数为每个训练文件的字符数</span></span><br><span class="line">    <span class="keyword">for</span> file_index, training_file <span class="keyword">in</span> <span class="built_in">enumerate</span>(training_file_list):</span><br><span class="line">        TRAINING_REAL_NUMS.append(training_file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])  <span class="comment"># 把正确的数字保存在列表中</span></span><br><span class="line">        <span class="comment"># 把所有的训练文件数据存储在一个矩阵中</span></span><br><span class="line">        training_matrix[file_index, :] = _make_matrix_by_file(TRAINING_PATH + training_file)</span><br><span class="line">    <span class="keyword">return</span> training_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_test_matrix_list</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取所有测试文件的数据并且转化为一个矩阵list</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    test_file_list = listdir(TEST_PATH)  <span class="comment"># 获取测试文件列表</span></span><br><span class="line">    test_matrix1_list = []</span><br><span class="line">    <span class="keyword">for</span> test_file <span class="keyword">in</span> test_file_list:</span><br><span class="line">        TEST_REAL_NUMS.append(test_file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 把所有的测试文件矩阵保存为一个list</span></span><br><span class="line">        test_matrix1_list.append(_make_matrix_by_file(TEST_PATH + test_file))</span><br><span class="line">    <span class="keyword">return</span> test_matrix1_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">training_matrix, test_matrix, k=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过训练矩阵对测试数据进行数字判断</span></span><br><span class="line"><span class="string">    :param training_matrix: 完整的训练矩阵</span></span><br><span class="line"><span class="string">    :param test_matrix: 测试数据</span></span><br><span class="line"><span class="string">    :param k: 取最相似的k个训练数据</span></span><br><span class="line"><span class="string">    :return: 分类结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    training_matrix_lines = training_matrix.shape[<span class="number">0</span>]  <span class="comment"># 训练矩阵的总行数，也就是训练文件数</span></span><br><span class="line">    test_matrix = tile(test_matrix, (training_matrix_lines, <span class="number">1</span>))  <span class="comment"># 对测试矩阵进行行复制，使其行数和训练矩阵行数一样多</span></span><br><span class="line">    distances = (((test_matrix - training_matrix) ** <span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">1</span>)) ** <span class="number">0.5</span>  <span class="comment"># 获取测试数据和所有的训练数据的差异值list</span></span><br><span class="line">    sorted_distances_index = distances.argsort()  <span class="comment"># 对差异值按从小到大进行排序，把排序前list的下标作为新list的值组成一个列表</span></span><br><span class="line"></span><br><span class="line">    num_count = &#123;&#125;  <span class="comment"># 记录某个数字的出现次数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># 根据下标获取到训练数据中该条数据所对应的真实数字（至于最终选不选该数字依赖于其出现的频率）</span></span><br><span class="line">        one_possible_number = TRAINING_REAL_NUMS[sorted_distances_index[i]]</span><br><span class="line">        <span class="comment"># 先获取该数字当前的出现次数（没有就为0），之后把当前次数加一作为新的出现次数</span></span><br><span class="line">        num_count[one_possible_number] = num_count.get(one_possible_number, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对数字出现的频次进行排序，从高到低</span></span><br><span class="line">    sorted_num_count = <span class="built_in">sorted</span>(num_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回出现频次最高的数字</span></span><br><span class="line">    <span class="keyword">return</span> sorted_num_count[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>():</span><br><span class="line">    training_matrix = generate_training_matrix()</span><br><span class="line">    test_matrix_list = generate_test_matrix_list()</span><br><span class="line">    right = <span class="number">0</span>  <span class="comment"># 正确的个数</span></span><br><span class="line">    total = <span class="number">0</span>  <span class="comment"># 总个数</span></span><br><span class="line">    <span class="keyword">for</span> idx, test_matrix <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_matrix_list):</span><br><span class="line">        real = TEST_REAL_NUMS[idx]</span><br><span class="line">        guess = classify(training_matrix, test_matrix, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> real == guess:</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        total += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;real: &#x27;</span>, real, <span class="string">&#x27;, guess: &#x27;</span>, guess)</span><br><span class="line"></span><br><span class="line">    fail = total - right</span><br><span class="line">    <span class="built_in">print</span>(fail, <span class="string">&#x27; fails, fail rate is &#x27;</span>, fail / total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure><p>上述代码的主要流程和操作已经在注释中做了比较详细的介绍，可能只有第67行的矩阵运算需要略作介绍，下面使用一个例子来介绍这里是怎么运算的。首先我们在66行中已经构造了一个和训练数据矩阵行数和列数都一样的测试数据矩阵，之后我们发现第67行一共做了如下几个操作：</p><ol><li>两个矩阵相减；</li><li>求矩阵平方（即矩阵相乘）；</li><li>对矩阵进行行求和；</li><li>对矩阵进行开方操作；</li></ol><p>我们先构造两个如下的矩阵：</p><pre><code>a = array([[2, 3, 4, 5],
           [3, 4, 5, 6],
           [4, 5, 6, 7]])

b = array([[3, 4, 5, 6],
           [0, 0, 0, 0],
           [9, 8, 7, 6]])
</code></pre><p>之后两个矩阵相减，得到如下结果，可见相减是矩阵中每个对应位置的元素都做了减法操作</p><pre><code>array([[-1, -1, -1, -1],
       [ 3,  4,  5,  6],
       [-5, -3, -1,  1]])
</code></pre><p>之后再对上面得到的矩阵做平方，也就是自己乘以自己，实际上是对每个对应位置的元素都做了乘法操作，平方结果如下</p><pre><code>array([[ 1,  1,  1,  1],
       [ 9, 16, 25, 36],
       [25,  9,  1,  1]])
</code></pre><p>之后对矩阵的每一行求和，使用方法 <code>matrix.sum(axis=1)</code> 可以简单做到，行求和后结果如下</p><pre><code>array([ 4, 86, 36])
</code></pre><p>最后再做一次开方，实际上就是对矩阵中的每个元素做开方，结果如下</p><pre><code>array([ 2.       ,  9.2736185,  6.       ])
</code></pre><p>下面是我在IPython中的操作过程<br><img src="/images/20170526/20170604103711.png"></p><p>完整的代码和数据可以从<a target="_blank" rel="noopener" href="https://github.com/RitterHou/learn_knn">这里</a>获取到。</p></div><div class="post-copyright"><div><strong>本文链接：</strong> <span title="k-近邻算法实现数字识别">https://www.nosuchfield.com/2017/05/26/K-Nearest-Neighbor-Algorithm-for-Digital-Recognition/</span></div><div><strong>版权声明： </strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处！</div></div><style>summary{cursor:pointer;margin-bottom:10px}summary:focus{outline:0}</style><script src="/js/code-enhancer.js"></script><script src="/js/pangu.min.js"></script><script>pangu.spacingPage()</script><script>function backToTop(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script defer src="https://cloud.umami.is/script.js" data-website-id="267e4aaf-8cb5-464d-b16c-89e66283e505"></script><div class="post-footer"><ul class="post-tag-list" itemprop="keywords"><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul><span onclick="backToTop()" class="top">返回顶部</span></div></article><footer><span>&copy; 2015 - 2025</span> <span class="author">Raymond</span> <span style="float:right"><span class="upyun">本网站由<a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"> <img src="/images/others/upyun.png"></a>提供CDN加速&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> <a class="filing" href="https://beian.miit.gov.cn/" target="_blank">苏ICP备15057335号</a> <a class="github" href="https://github.com/RitterHou" target="_blank">GitHub</a></span></footer></div></body></html>