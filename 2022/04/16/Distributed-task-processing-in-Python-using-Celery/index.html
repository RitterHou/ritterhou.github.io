<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><title>使用Celery实现Python分布式任务处理 - 侯锐的思考与分享</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="keywords" content="分布式系统,消息队列,Python"><meta name="description" content="&lt;p&gt;Celery是一个任务队列，它可以实现跨进程和机器的分布式任务处理。任务队列的输入端会输入各种任务（task），这些任务会在输出端由worker进行处理，这些任务会由客户端通过发送消息的方式交给broker，随后broker把任务分发给worker。&lt;/p&gt;
&lt;h2 id=&#34;安装组件&#34;&gt;&lt;a href=&#34;#安装组件&#34; class=&#34;headerlink&#34; title=&#34;安装组件&#34;&gt;&lt;/a&gt;安装组件&lt;/h2&gt;&lt;p&gt;本文使用到的组件版本&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;组件&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Python&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.7.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Celery&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.4.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Redis&lt;/td&gt;
&lt;td align=&#34;"><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="侯锐的思考与分享" type="application/atom+xml"></head><body><div class="container"><header class="header"><div class="blog-title"><a href="/" class="logo">侯锐的思考与分享</a><div class="subtitle"></div></div><nav class="navbar"><ul class="menu"><li class="menu-item"><a href="/" class="menu-item-link" data-no-instant>主页</a></li><li class="menu-item"><a href="/atom.xml" class="menu-item-link" data-no-instant>订阅</a></li><li class="menu-item"><a href="/search" class="menu-item-link" data-no-instant>搜索</a></li></ul></nav></header><article class="post"><div class="post-title"><h1 class="article-title">使用Celery实现Python分布式任务处理</h1></div><div class="post-meta"><span class="post-time">2022-04-16</span></div><div class="post-content"><p>Celery是一个任务队列，它可以实现跨进程和机器的分布式任务处理。任务队列的输入端会输入各种任务（task），这些任务会在输出端由worker进行处理，这些任务会由客户端通过发送消息的方式交给broker，随后broker把任务分发给worker。</p><h2 id="安装组件"><a href="#安装组件" class="headerlink" title="安装组件"></a>安装组件</h2><p>本文使用到的组件版本</p><table><thead><tr><th align="left">组件</th><th align="left">版本</th></tr></thead><tbody><tr><td align="left">Python</td><td align="left">2.7.16</td></tr><tr><td align="left">Celery</td><td align="left">4.4.7</td></tr><tr><td align="left">Redis</td><td align="left">6.2.4</td></tr><tr><td align="left">redis-py</td><td align="left">3.2.1</td></tr></tbody></table><p>首先我们需要安装celery和Redis的依赖包</p><pre><code>pip install celery==4.4.7
pip install redis==3.2.1
</code></pre><p>Celery支持多种类型的broker，在这里我们主要使用Redis作为Celery的broker，关于Redis的安装和使用可以参考我之前的文章<a href="/2018/11/29/Redis-failover/">Redis failover</a>。</p><h2 id="构建应用"><a href="#构建应用" class="headerlink" title="构建应用"></a>构建应用</h2><p>我们首先创建如下的目录结构（本文的示例代码都放在了<a target="_blank" rel="noopener" href="https://github.com/RitterHou/celery_demo">GitHub上面</a>）</p><pre><code>.
├── run.py
└── search
    ├── __init__.py
    ├── config.py
    └── tasks.py
</code></pre><h3 id="创建celery应用"><a href="#创建celery应用" class="headerlink" title="创建celery应用"></a>创建celery应用</h3><p><code>search/config.py</code>包含了一些celery的配置文件，具体配置如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置任务模块</span></span><br><span class="line">include = [<span class="string">&#x27;search.tasks&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对指定的任务使用一个特定的队列进行路由，该任务在发送时会被发送到该指定队列中</span></span><br><span class="line"><span class="comment"># 未指定队列的任务默认发送到一个名叫celery的队列</span></span><br><span class="line">task_routes = &#123;</span><br><span class="line">    <span class="string">&#x27;search.tasks.sort_list&#x27;</span>: &#123;<span class="string">&#x27;queue&#x27;</span>: <span class="string">&#x27;queue_1&#x27;</span>&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在<code>search/__init__.py</code>中我们利用如上的配置信息初始化一个celery的app</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> Celery</span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> config</span><br><span class="line"></span><br><span class="line">app = Celery(<span class="string">&#x27;search&#x27;</span>, broker=<span class="string">&#x27;redis://127.0.0.1:6379/1&#x27;</span>, backend=<span class="string">&#x27;redis://127.0.0.1:6379/1&#x27;</span>)</span><br><span class="line">app.config_from_object(config)</span><br></pre></td></tr></table></figure><h3 id="创建celery任务"><a href="#创建celery任务" class="headerlink" title="创建celery任务"></a>创建celery任务</h3><p>如上我们创建了一个celery的app，该app使用Redis作为broker和backend，之后我们在<code>search/tasks.py</code>中创建任务</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> app</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.task</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_url</span>(<span class="params">url</span>):</span><br><span class="line">    r = requests.get(url)</span><br><span class="line">    <span class="keyword">return</span> r.status_code</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.task</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x + y</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.task</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sort_list</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(data)</span><br></pre></td></tr></table></figure><h2 id="启动celery的worker"><a href="#启动celery的worker" class="headerlink" title="启动celery的worker"></a>启动celery的worker</h2><p>写完代码之后我们在项目根目录执行如下命令启动一个celery的worker</p><pre><code>celery -A search worker -Q queue_1,celery -l info
</code></pre><p>其中，<code>-A</code>是<code>--app</code>的简写，代表启动的应用；<code>worker</code>表示当前命令是要启动一个celery的worker；<code>-Q queue_1,celery</code>表示当前worker监听<code>queue_1</code>和<code>celery</code>队列，不指定的话默认使用一个名叫<code>celery</code>的队列；<code>-l info</code>表示日志的级别。启动后输出如下内容，代表celery的worker已经成功启动了</p><pre><code>-------------- celery@Mac v4.4.7 (cliffs)
--- ***** ----- 
-- ******* ---- Darwin-19.6.0-x86_64-i386-64bit 2022-04-15 14:30:06
- *** --- * --- 
- ** ---------- [config]
- ** ---------- .&gt; app:         search:0x104604090
- ** ---------- .&gt; transport:   redis://127.0.0.1:6379/1
- ** ---------- .&gt; results:     redis://127.0.0.1:6379/1
- *** --- * --- .&gt; concurrency: 4 (prefork)
-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)
--- ***** ----- 
-------------- [queues]
                .&gt; celery           exchange=celery(direct) key=celery
                .&gt; queue_1          exchange=queue_1(direct) key=queue_1

[tasks]
. search.tasks.add
. search.tasks.search_url
. search.tasks.sort_list

[2022-04-15 14:30:06,838: INFO/MainProcess] Connected to redis://127.0.0.1:6379/1
[2022-04-15 14:30:06,855: INFO/MainProcess] mingle: searching for neighbors
[2022-04-15 14:30:07,930: INFO/MainProcess] mingle: all alone
[2022-04-15 14:30:07,983: INFO/MainProcess] celery@Mac ready.
</code></pre><h2 id="发送任务"><a href="#发送任务" class="headerlink" title="发送任务"></a>发送任务</h2><p>之后我们在<code>run.py</code>中发送celery任务并交给worker执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> search.tasks <span class="keyword">import</span> search_url, add, sort_list, get_redis_keys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    logging.info(search_url.delay(<span class="string">&#x27;https://www.jd.com&#x27;</span>).get(<span class="number">5</span>))</span><br><span class="line">    logging.info(add.delay(<span class="number">1.5</span>, <span class="number">3.5</span>).get(<span class="number">5</span>))</span><br><span class="line">    data = [<span class="number">5</span>, <span class="number">86</span>, <span class="number">59</span>, <span class="number">17</span>, <span class="number">24</span>, <span class="number">92</span>, <span class="number">38</span>, <span class="number">95</span>, <span class="number">13</span>, <span class="number">89</span>, <span class="number">63</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">60</span>, <span class="number">6</span>]</span><br><span class="line">    logging.info(sort_list.delay(data).get(<span class="number">5</span>))</span><br><span class="line">    <span class="comment"># 将任务发送到指定队列，如果该队列没有worker监听，则此任务不会执行，5秒后超时</span></span><br><span class="line">    logging.info(add.apply_async((<span class="number">1.5</span>, <span class="number">3.5</span>), queue=<span class="string">&#x27;queue_2&#x27;</span>).get(<span class="number">5</span>))</span><br></pre></td></tr></table></figure><p>只需要在原有的方法基础上添加一个<code>delay</code>方法，就可以实现任务的发送并交给worker执行，非常简单。<code>delay</code>是<code>apply_async</code>方法的简化版，在<code>apply_async</code>方法中我们还可以指定该任务的发送队列，以及一些其它的配置。</p><p><code>apply_async</code>方法的返回值是一个<a target="_blank" rel="noopener" href="https://docs.celeryq.dev/en/v4.4.7/reference/celery.result.html#celery.result.AsyncResult">AsyncResult</a>类型，该类型的对象可以获取任务的信息，例如<code>successful()</code>和<code>failed()</code>方法可以获取到该任务是否执行成功，<code>id</code>和<code>state</code>属性可以获取到该任务的id和状态。如上所示，<code>get()</code>方法可以获取到该任务的返回值，为了避免卡死可以在执行时添加<code>get(timeout)</code>方法的超时时间。</p><h3 id="通过设置队列实现路由功能"><a href="#通过设置队列实现路由功能" class="headerlink" title="通过设置队列实现路由功能"></a>通过设置队列实现路由功能</h3><p>celery可以通过设置队列来实现<a target="_blank" rel="noopener" href="https://docs.celeryq.dev/en/v4.4.7/userguide/routing.html">任务的路由</a>。假设我们有三个任务，它们发送任务的队列设置分别为q1，q2，q3。同时我们还有三个worker，它们的队列设置分别为q1，q1，q2,q3。那么任务1将会发送到worker1或者worker2上执行，而任务2和任务3都会在worker3上执行，通过队列就可以实现把任务发送到指定的worker上执行的功能。</p><table><thead><tr><th align="left">worker</th><th align="left">queue</th><th align="left">task</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">2</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">3</td><td align="left">2,3</td><td align="left">2,3</td></tr></tbody></table><p>由上可见，task是和queue绑定的，一个task只能发送到一个指定的queue。而一个worker既可以监听多个queue，也可以多个worker监听一个queue，前者可以实现worker能力的扩展，后者可以实现任务的多负载均衡。</p><h2 id="celery的监控"><a href="#celery的监控" class="headerlink" title="celery的监控"></a>celery的监控</h2><p><a target="_blank" rel="noopener" href="https://docs.celeryq.dev/en/v4.4.7/userguide/monitoring.html#flower-real-time-celery-web-monitor">Flower</a>是一个celery的网页监控和管理工具，使用前需要先安装</p><pre><code>pip install flower==0.9.2
</code></pre><p>之后我们可以启动它</p><pre><code>celery -A search flower --port=5555 -Q queue_1,celery -l info
</code></pre><p>随后访问<a target="_blank" rel="noopener" href="http://127.0.0.1:5555/">http://127.0.0.1:5555</a>就可以查看celery的监控信息了，我们也可以在网页上对celery进行一些管理操作。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://docs.celeryq.dev/en/v4.4.7/index.html">https://docs.celeryq.dev/en/v4.4.7/index.html</a><br><a target="_blank" rel="noopener" href="https://github.com/RitterHou/celery_demo">https://github.com/RitterHou/celery_demo</a></p></div><div class="post-copyright"><div><strong>本文链接：</strong> <span title="使用Celery实现Python分布式任务处理">https://www.nosuchfield.com/2022/04/16/Distributed-task-processing-in-Python-using-Celery/</span></div><div><strong>版权声明： </strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处！</div></div><style>summary{cursor:pointer;margin-bottom:10px}summary:focus{outline:0}</style><script src="/js/code-enhancer.js"></script><script src="/js/pangu.min.js"></script><script>pangu.spacingPage()</script><script>function backToTop(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script defer src="https://cloud.umami.is/script.js" data-website-id="267e4aaf-8cb5-464d-b16c-89e66283e505"></script><div class="post-footer"><ul class="post-tag-list" itemprop="keywords"><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/" rel="tag">分布式系统</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" rel="tag">消息队列</a></li></ul><span onclick="backToTop()" class="top">返回顶部</span></div></article><footer><span>&copy; 2015 - 2025</span> <span class="author">Raymond</span> <span style="float:right"><span class="upyun">本网站由<a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"> <img src="/images/others/upyun.png"></a>提供CDN加速&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> <a class="filing" href="https://beian.miit.gov.cn/" target="_blank">苏ICP备15057335号</a> <a class="github" href="https://github.com/RitterHou" target="_blank">GitHub</a></span></footer></div></body></html>