<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><title>Spark学习笔记 - 侯锐的思考与分享</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="keywords" content="大数据,分布式系统,分布式计算,Spark"><meta name="description" content="&lt;h3 id=&#34;安装Spark&#34;&gt;&lt;a href=&#34;#安装Spark&#34; class=&#34;headerlink&#34; title=&#34;安装Spark&#34;&gt;&lt;/a&gt;安装Spark&lt;/h3&gt;&lt;p&gt;Spark的运行依赖&lt;a href=&#34;https://en.wikipedia.org/wiki/Java_virtual_machine&#34;&gt;JVM&lt;/a&gt;，所以你需要一台安装了JVM的机器。&lt;/p&gt;
&lt;p&gt;首先从 &lt;a href=&#34;https://spark.apache.org/downloads.html&#34;&gt;https://spark.apache.org/downloads.html&lt;/a&gt; 下载Spark程序包并解压，解压后的目录如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;➜  spark-2.3.0-bin-hadoop2.7 ll
total 112
-rw-r--r--@   1 hourui  staff  18045  2 23  2018 LICENSE
-rw-r--r--@   1 hourui  staff  24913  2 23  2018 NOTICE
drwxr-xr-x@   3"><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="侯锐的思考与分享" type="application/atom+xml"></head><body><div class="container"><header class="header"><div class="blog-title"><a href="/" class="logo">侯锐的思考与分享</a><div class="subtitle"></div></div><nav class="navbar"><ul class="menu"><li class="menu-item"><a href="/" class="menu-item-link" data-no-instant>主页</a></li><li class="menu-item"><a href="/atom.xml" class="menu-item-link" data-no-instant>订阅</a></li><li class="menu-item"><a href="/search" class="menu-item-link" data-no-instant>搜索</a></li></ul></nav></header><article class="post"><div class="post-title"><h1 class="article-title">Spark学习笔记</h1></div><div class="post-meta"><span class="post-time">2019-02-15</span></div><div class="post-content"><h3 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h3><p>Spark的运行依赖<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Java_virtual_machine">JVM</a>，所以你需要一台安装了JVM的机器。</p><p>首先从 <a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a> 下载Spark程序包并解压，解压后的目录如下</p><pre><code>➜  spark-2.3.0-bin-hadoop2.7 ll
total 112
-rw-r--r--@   1 hourui  staff  18045  2 23  2018 LICENSE
-rw-r--r--@   1 hourui  staff  24913  2 23  2018 NOTICE
drwxr-xr-x@   3 hourui  staff     96  2 23  2018 R
-rw-r--r--@   1 hourui  staff   3809  2 23  2018 README.md
-rw-r--r--@   1 hourui  staff    187  2 23  2018 RELEASE
drwxr-xr-x@  29 hourui  staff    928  2 23  2018 bin
drwxr-xr-x@   9 hourui  staff    288  2 23  2018 conf
drwxr-xr-x@   5 hourui  staff    160  2 23  2018 data
drwxr-xr-x@   4 hourui  staff    128  2 23  2018 examples
drwxr-xr-x@ 229 hourui  staff   7328  2 23  2018 jars
drwxr-xr-x@   3 hourui  staff     96  2 23  2018 kubernetes
drwxr-xr-x@  40 hourui  staff   1280  2 23  2018 licenses
drwxr-xr-x    5 hourui  staff    160  2 15 16:40 logs
drwxr-xr-x@  16 hourui  staff    512  2 23  2018 python
drwxr-xr-x@  24 hourui  staff    768  2 23  2018 sbin
drwxr-xr-x    2 hourui  staff     64  2 15 16:38 work
drwxr-xr-x@   3 hourui  staff     96  2 23  2018 yarn
</code></pre><p>其中 <code>sbin</code> 文件夹中保存了管理集群相关的脚本，而 <code>bin</code> 文件夹中则保存了Spark任务相关的脚本。Spark的任务支持R、Python、Java和Scala编写，比较常见的是使用Java或Scala。</p><p>Spark的启动分为local和集群，而<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/#launching-on-a-cluster">集群</a>又有以下几种创建方式</p><ul><li>Standalone Deploy Mode</li><li>Apache Mesos</li><li>Hadoop YARN</li><li>Kubernetes</li></ul><p>Spark支持Python、Scala和R的shell，同时也支持Python、Scala、R和Java的<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/submitting-applications.html">submit to cluster</a>。</p><h3 id="启动和停止Standalone集群"><a href="#启动和停止Standalone集群" class="headerlink" title="启动和停止Standalone集群"></a>启动和停止<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/spark-standalone.html">Standalone</a>集群</h3><p>Spark中的节点分为master节点和worker节点，worker节点受master节点管理实现高可用，master节点本身可以通过Zookeeper实现高可用。管理Spark集群的相关脚本都在 <code>sbin</code> 文件夹中</p><h4 id="启动master节点"><a href="#启动master节点" class="headerlink" title="启动master节点"></a>启动master节点</h4><pre><code>➜  sbin ./start-master.sh --host 127.0.0.1
starting org.apache.spark.deploy.master.Master, logging to /Users/hourui/spark-2.3.0-bin-hadoop2.7/logs/spark-hourui-org.apache.spark.deploy.master.Master-1-Mac.out
</code></pre><p>查看master进程状态</p><pre><code>➜  sbin ps -ef | grep spark
501 34593     1   0  5:48下午 ttys000    0:03.98 /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java -cp /Users/hourui/spark-2.3.0-bin-hadoop2.7/conf/:/Users/hourui/spark-2.3.0-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host Mac --port 7077 --webui-port 8080 --host 127.0.0.1
</code></pre><h4 id="启动worker节点"><a href="#启动worker节点" class="headerlink" title="启动worker节点"></a>启动worker节点</h4><p>在上面的master进程状态中我们看到了master进程的host是<code>127.0.0.1</code>，port是7077，我们根据master的host和port构建url <code>spark://&lt;host&gt;:&lt;port&gt;</code>，worker以此url作为参数启动。</p><pre><code>➜  sbin ./start-slave.sh spark://127.0.0.1:7077
starting org.apache.spark.deploy.worker.Worker, logging to /Users/hourui/spark-2.3.0-bin-hadoop2.7/logs/spark-hourui-org.apache.spark.deploy.worker.Worker-1-Mac.out
</code></pre><p>接下来我们使用命令 <code>ps</code> 查看集群的情况</p><pre><code>➜  ps -ef | grep spark
501 34593     1   0  5:48下午 ttys000    0:05.54 /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java -cp /Users/hourui/spark-2.3.0-bin-hadoop2.7/conf/:/Users/hourui/spark-2.3.0-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host Mac --port 7077 --webui-port 8080 --host 127.0.0.1
501 34691     1   0  5:51下午 ttys000    0:05.14 /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java -cp /Users/hourui/spark-2.3.0-bin-hadoop2.7/conf/:/Users/hourui/spark-2.3.0-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://127.0.0.1:7077
</code></pre><p>可以看到master和worker进程都正常启动了。此外，master和worker还各自提供了监听在8080和8081端口的webui，我们可以在浏览器中访问<a target="_blank" rel="noopener" href="http://127.0.0.1:8080/">http://127.0.0.1:8080/</a>和<a target="_blank" rel="noopener" href="http://127.0.0.1:8081/">http://127.0.0.1:8081/</a>观察master和worker的状态。</p><h4 id="停止master和worker进程"><a href="#停止master和worker进程" class="headerlink" title="停止master和worker进程"></a>停止master和worker进程</h4><pre><code>➜  sbin ./stop-slave.sh
stopping org.apache.spark.deploy.worker.Worker
➜  sbin ./stop-master.sh
stopping org.apache.spark.deploy.master.Master
</code></pre><p>之后我们再使用 <code>ps</code> 命令就会发现spark进程已经都没有了。</p><h3 id="运行-Spark-应用"><a href="#运行-Spark-应用" class="headerlink" title="运行 Spark 应用"></a>运行 Spark 应用</h3><p>就Scala而言，目前我们有着三种提交任务的方式：</p><p>1. 使用shell（这里启动的是Scala的shell，当然 <code>./pyspark</code> 和 <code>./sparkR</code> 也能启动Python和R的shell）</p><pre><code>./spark-shell --master spark://127.0.0.1:7077
</code></pre><p>如果不设置master则默认为local模式</p><p>2. 直接在代码中设置master为local，之后运行程序，程序会在本地运行</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">val</span> inputFile = <span class="string">&quot;/Users/hourui/Desktop/test_go.go&quot;</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;learn_spark&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> textFile = sc.textFile(inputFile)</span><br><span class="line">        <span class="keyword">val</span> wordCount = textFile.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>)).reduceByKey((a, b) =&gt; a + b)</span><br><span class="line">        wordCount.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3. 写完代码并打包，之后使用 <code>./spark-submit</code> 提交程序到Spark集群。</p><p>例如对于上面这个程序，我们把其放在maven项目中，项目结构如下：</p><pre><code>.
├── pom.xml
└── src
    ├── main
    │   ├── java
    │   │   └── me
    │   │       └── hourui
    │   │           └── Test.scala
    │   └── resources
    └── test
        └── java
</code></pre><p>我们的代码位于<code>Test.scala</code>，使用命令 <code>mvn clean package</code> 打包，得到文件 <code>learn_spark-1.0-SNAPSHOT.jar</code>。之后使用命令提交任务到集群</p><pre><code>➜  bin ./spark-submit --master spark://127.0.0.1:7077 \
--deploy-mode cluster \
--class me.hourui.Test \
/Users/hourui/IdeaProjects/learn_spark/target/learn_spark-1.0-SNAPSHOT.jar
</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="http://chant00.com/2017/07/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">http://chant00.com/2017/07/28/Spark学习笔记/</a><br><a target="_blank" rel="noopener" href="https://book.douban.com/subject/26616244/">Spark快速大数据分析</a></p></div><div class="post-copyright"><div><strong>本文链接：</strong> <span title="Spark学习笔记">https://www.nosuchfield.com/2019/02/15/Spark-study-notes/</span></div><div><strong>版权声明： </strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处！</div></div><style>summary{cursor:pointer;margin-bottom:10px}summary:focus{outline:0}</style><script src="/js/code-enhancer.js"></script><script src="/js/pangu.min.js"></script><script>pangu.spacingPage()</script><script>function backToTop(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script defer src="https://cloud.umami.is/script.js" data-website-id="267e4aaf-8cb5-464d-b16c-89e66283e505"></script><div class="post-footer"><ul class="post-tag-list" itemprop="keywords"><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/" rel="tag">分布式系统</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/" rel="tag">分布式计算</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul><span onclick="backToTop()" class="top">返回顶部</span></div></article><footer><span>&copy; 2015 - 2025</span> <span class="author">Raymond</span> <span style="float:right"><span class="upyun">本网站由<a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"> <img src="/images/others/upyun.png"></a>提供CDN加速&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> <a class="filing" href="https://beian.miit.gov.cn/" target="_blank">苏ICP备15057335号</a> <a class="github" href="https://github.com/RitterHou" target="_blank">GitHub</a></span></footer></div></body></html>