<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><title>聊聊Elasticsearch中的文本分析 - 侯锐的思考与分享</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="keywords" content="nlp,分词,自然语言处理,ElasticSearch"><meta name="description" content="&lt;blockquote&gt;
&lt;p&gt;本文使用到的是Elasticsearch-7.5.2与Lucene-8.3.0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Elasticsearch中的string类型存在着两种不同的类型。一种是结构化的数据称为keyword，elasticsearch在索引时不会对这类数据做任何的处理；另一种是非结构化的数据称为text，elasticsearch在对这类型的数据做索引之前，会先对原始的string数据做一定的分析处理，之后得到一些结构化的数据结果，然后针对这些结构化的数据做进一步的处理。&lt;/p&gt;
&lt;p&gt;例如一个field的类型是text，则Elasticsearch在对这个字段进行索引的时候，会先把这个field的值分词为tokens之后再保存这些tokens。而在查询时如果我们使用match查询，则Elasticsearch会对match查询的值进行分词得到tokens，之后才会使用这些tokens进行真正的查询操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/20200428/2.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图显示了Elasticsear"><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="侯锐的思考与分享" type="application/atom+xml"></head><body><div class="container"><header class="header"><div class="blog-title"><a href="/" class="logo">侯锐的思考与分享</a><div class="subtitle"></div></div><nav class="navbar"><ul class="menu"><li class="menu-item"><a href="/" class="menu-item-link" data-no-instant>主页</a></li><li class="menu-item"><a href="/atom.xml" class="menu-item-link" data-no-instant>订阅</a></li><li class="menu-item"><a href="/search" class="menu-item-link" data-no-instant>搜索</a></li></ul></nav></header><article class="post"><div class="post-title"><h1 class="article-title">聊聊Elasticsearch中的文本分析</h1></div><div class="post-meta"><span class="post-time">2020-04-28</span></div><div class="post-content"><blockquote><p>本文使用到的是Elasticsearch-7.5.2与Lucene-8.3.0</p></blockquote><p>Elasticsearch中的string类型存在着两种不同的类型。一种是结构化的数据称为keyword，elasticsearch在索引时不会对这类数据做任何的处理；另一种是非结构化的数据称为text，elasticsearch在对这类型的数据做索引之前，会先对原始的string数据做一定的分析处理，之后得到一些结构化的数据结果，然后针对这些结构化的数据做进一步的处理。</p><p>例如一个field的类型是text，则Elasticsearch在对这个字段进行索引的时候，会先把这个field的值分词为tokens之后再保存这些tokens。而在查询时如果我们使用match查询，则Elasticsearch会对match查询的值进行分词得到tokens，之后才会使用这些tokens进行真正的查询操作。</p><p><img src="/images/20200428/2.png"></p><p>上图显示了Elasticsearch中结构化数据与非结构化数据的保存与查询流程，需要注意的是，Elasticsearch最终只保存结构化的数据，不会保存非结构化的数据。因此我们可以保存一个text类型的数据到ES中，之后使用term方式来进行查询，这也是可以的。</p><h2 id="Elasticsearch-Text-Analysis"><a href="#Elasticsearch-Text-Analysis" class="headerlink" title="Elasticsearch Text Analysis"></a>Elasticsearch Text Analysis</h2><p>从非结构化字符串转化为结构化字符串的过程在Elasticsearch中称为Text analysis，Text analysis流程又分为如下的几个子流程：</p><ul><li>使用零个或者多个Character filters，对原始的文本进行一些处理</li><li>使用唯一的Tokenizer，对原始的文本进行分词处理，得到一些tokens&#x2F;terms</li><li>使用零个或者多个Token filters，对上一步的tokens继续进行处理，例如合并同义词等</li></ul><p>我们可以使用如下命令在es中给一个text字段设置指定的analyzer</p><pre><code>PUT my_index
&#123;
    &quot;mappings&quot;: &#123;
        &quot;properties&quot;: &#123;
            &quot;title&quot;: &#123;
                &quot;type&quot;: &quot;text&quot;,
                &quot;analyzer&quot;: &quot;whitespace&quot;,
                &quot;search_analyzer&quot;: &quot;simple&quot;  # 可选，默认是analyzer的值
            &#125;
        &#125;
    &#125;
&#125;
</code></pre><p>以上的analyzer为字段在索引时使用，在查询时字段按照如下优先级使用analyzer进行分析：</p><ol><li>查询时手动指定的analyzer</li><li>被查询字段的search_analyzer属性</li><li>索引的default的analyzer的type</li><li>被查询字段的analyzer属性</li></ol><p>es中包含了很多内置的analyzer，例如上面的simple和whitespace，这些analyzer能够做到开箱即用。此外es中还包含了很多内置的filter、tokenizer和char_filter，我们也可以使用这些基础组件构建出一些自定义的analyzer，如下就是一个例子</p><pre><code>PUT my_index
&#123;
    &quot;settings&quot;: &#123;
        &quot;analysis&quot;: &#123;
            &quot;analyzer&quot;: &#123;
                &quot;my_custom_analyzer&quot;: &#123;
                    &quot;type&quot;: &quot;custom&quot;,  # 这里设置为custom表示自定义一个analyzer
                    &quot;tokenizer&quot;: &quot;standard&quot;,
                    &quot;char_filter&quot;: [
                        &quot;html_strip&quot;
                    ],
                    &quot;filter&quot;: [
                        &quot;lowercase&quot;,
                        &quot;asciifolding&quot;
                    ]
                &#125;
            &#125;
        &#125;
    &#125;
&#125;
</code></pre><p>es的自定义analyzer功能给文本处理提供了极大的方便，在内置的analyzer无法满足我们的文本分析需求时，我们可以使用内置的character_filter、tokenizer和token_filter来构建符合我们需要的自定义analyzer，这已经十分强大了。但是在有的时候，即使是由character_filter、tokenizer和token_filter自定义的analyzer也无法满足我们的文本分析要求，这时候就需要用到elasticsearch的Analysis插件体系了。</p><h2 id="Elasticsearch-Plugins"><a href="#Elasticsearch-Plugins" class="headerlink" title="Elasticsearch Plugins"></a>Elasticsearch Plugins</h2><p>Elasticsearch包含了很多官方以及社区贡献的plugin，这些插件都可以在es中进行安装或删除：</p><pre><code>./bin/elasticsearch-plugin install analysis-smartcn
./bin/elasticsearch-plugin remove analysis-smartcn
# 或者下载了安装包之后手动的安装plugin
./bin/elasticsearch-plugin install file:///home/elasticsearch/elasticsearch-analysis-ik-7.5.2.zip
</code></pre><p>在plugin中有一类叫做Analysis Plugin，该类plugin可以帮助es扩展文本分析的能力。下面我们来详细了解一下如何实现自己的analysis plugin和创建新的analyzer、tokenizer等等组件，以及如何在es中使用这些我们自己创建的插件。</p><p>Elasticsearch的分词器插件本质上使用的是Lucene的分词器插件，所以我们需要从两个部分来介绍Elasticsearch的分词器插件</p><ol><li>Elasticsearch如何调用Lucene的分词器插件</li><li>Lucene如何自定义一个分词器插件</li></ol><h3 id="Elasticsearch调用分词器插件"><a href="#Elasticsearch调用分词器插件" class="headerlink" title="Elasticsearch调用分词器插件"></a>Elasticsearch调用分词器插件</h3><p>如果我们需要写一个能够让Elasticsearch进行调用的插件，首先我们要实现一个继承自 <code>org.elasticsearch.plugins.Plugin</code> 的类，并且该类需要实现 <code>org.elasticsearch.plugins.AnalysisPlugin</code> 接口<sup>参考(一)</sup>。你可以重写 <code>AnalysisPlugin</code> 接口的 <code>getAnalyzers</code> 方法，该方法最终会返回一个 <code>org.apache.lucene.analysis.Analyzer</code> 的对象，该对象就是一个实现了自定义分词操作的Lucene类型对象；你也可以重写 <code>AnalysisPlugin</code> 接口的 <code>getTokenizers</code> 方法，该方法最终会返回一个 <code>org.apache.lucene.analysis.Tokenizer</code> 的对象，该对象实现了你自定义的分词逻辑。关于这些自定义Lucene分词器的创建会在下一节进行详细介绍。<code>AnalysisPlugin</code>接口中还包含了一些其它的方法，更多信息可以参考<a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch/blob/v7.5.2/server/src/main/java/org/elasticsearch/plugins/AnalysisPlugin.java">该类的源代码</a>。</p><p>下面我们看一个实际的例子，首先我们创建AnalysisQianmiPlugin类，它继承Plugin类且实现了AnalysisPlugin接口，它还重写了AnalysisPlugin接口的<code>getAnalyzers</code>方法，具体如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AnalysisQianmiPlugin</span> <span class="keyword">extends</span> <span class="title class_">Plugin</span> <span class="keyword">implements</span> <span class="title class_">AnalysisPlugin</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, AnalysisModule.AnalysisProvider&lt;AnalyzerProvider&lt;? <span class="keyword">extends</span> <span class="title class_">Analyzer</span>&gt;&gt;&gt; getAnalyzers() &#123;</span><br><span class="line">        Map&lt;String, AnalysisModule.AnalysisProvider&lt;AnalyzerProvider&lt;? <span class="keyword">extends</span> <span class="title class_">Analyzer</span>&gt;&gt;&gt; extra = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        extra.put(<span class="string">&quot;qm_standard&quot;</span>, QianmiStandardAnalyzerProvider::<span class="keyword">new</span>);</span><br><span class="line">        extra.put(<span class="string">&quot;sub&quot;</span>, QianmiSubAnalyzerProvider::<span class="keyword">new</span>);</span><br><span class="line">        <span class="keyword">return</span> extra;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的类中我们定义了两个ES的Analyzer，它们的名字分别为qm_standard和sub，我们选择qm_standard分析器的provider即QianmiStandardAnalyzerProvider<sup>参考(二)</sup>来做进一步的了解，QianmiStandardAnalyzerProvider的实现如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">QianmiStandardAnalyzerProvider</span> <span class="keyword">extends</span> <span class="title class_">AbstractIndexAnalyzerProvider</span>&lt;Analyzer&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Analyzer analyzer;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">QianmiStandardAnalyzerProvider</span><span class="params">(IndexSettings indexSettings, Environment env, String name, Settings settings)</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>(indexSettings, name, settings);</span><br><span class="line">        analyzer = <span class="keyword">new</span> <span class="title class_">QianmiStandardAnalyzer</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Analyzer <span class="title function_">get</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> analyzer;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>QianmiStandardAnalyzerProvider继承自Elasticsearch的AbstractIndexAnalyzerProvider类，并且重写了 <code>public Analyzer get()</code> 方法，该方法返回一个 <code>org.apache.lucene.analysis.Analyzer</code> 对象，具体Analyzer类的实现会在后一节中做进一步介绍。在AnalyzerProvider类的构造方法中我们还传入了Elasticsearch的一些属性，包括了</p><table><thead><tr><th align="left">属性</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">IndexSettings</td><td align="left">Elasticsearch中索引的信息</td></tr><tr><td align="left">Environment</td><td align="left">Elasticsearch的环境属性</td></tr><tr><td align="left">AnalyzerName</td><td align="left">当前分析器的名称</td></tr><tr><td align="left">Settings</td><td align="left">当前分析器的属性</td></tr></tbody></table><p>上面传入的这些属性我们可以在Lucene的分词过程中根据需要进行使用。</p><p>在创建了上面的类并且实现了相应的分词逻辑之后，我们可以对代码进行打包得到一个jar。之后我们把该jar包和一个ES插件的<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.5/plugin-authors.html#_plugin_descriptor_file">描述文件</a>放在同一个文件夹中，然后把这个文件夹打包为一个zip文件，得到的这个zip文件就是一个ES的plugin了。</p><p>Elasticsearch的描述文件固定为 <code>plugin-descriptor.properties</code>，它一般包含了如下的内容</p><pre><code># 插件的描述信息
description=$&#123;project.description&#125;
# 插件的版本号
version=$&#123;project.version&#125;
# 插件的名称
name=$&#123;elasticsearch.plugin.name&#125;
# 插件的全限定路径，就是之前继承自Plugin的那个类的全限定路径
classname=$&#123;elasticsearch.plugin.classname&#125;
# 使用的Java版本信息
java.version=$&#123;elasticsearch.plugin.java.version&#125;
# 插件对应的Elasticsearch的版本
elasticsearch.version=$&#123;elasticsearch.version&#125;
</code></pre><p>随后我们可以使用如下命令将该pulgin安装到Elasticsearch中</p><pre><code>./bin/elasticsearch-plugin install file:///Users/derobukal/elasticsearch-analysis-ansj/target/releases/elasticsearch-analysis-qianmi-7.5.2-release.zip
</code></pre><h3 id="Lucene自定义分词器插件"><a href="#Lucene自定义分词器插件" class="headerlink" title="Lucene自定义分词器插件"></a>Lucene自定义分词器插件</h3><p>在上面Elasticsearch调用分词插件的介绍中我们已经知道了Elasticsearch最终会需要一个 <code>org.apache.lucene.analysis.Analyzer</code> 的对象来实现真正的分词操作，下面我们就来了解一下我们如何定义一个这样Lucene的类。如下就是一个例子，QianmiStandardAnalyzer继承自Analyzer并且重写了其<code>createComponents</code>方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">QianmiStandardAnalyzer</span> <span class="keyword">extends</span> <span class="title class_">Analyzer</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> TokenStreamComponents <span class="title function_">createComponents</span><span class="params">(String fieldName)</span> &#123;</span><br><span class="line">        <span class="type">Tokenizer</span> <span class="variable">tokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">QianmiStandardTokenizer</span>();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TokenStreamComponents</span>(tokenizer);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法createComponents返回了一个<a target="_blank" rel="noopener" href="https://lucene.apache.org/core/8_3_0/core/org/apache/lucene/analysis/TokenStream.html">TokenStream</a>，TokenStream会协助生成token。在上面的方法中我们使用到了一个名为<code>QianmiStandardTokenizer</code>的类，这个tokenizer实现了最终的分词逻辑。我们也可以看到我们在Elasticsearch中所提到的character_filter、tokenizer和token_filter其实都是Lucene中的概念，在这里我们只用到了tokenizer，其实在<code>createComponents</code>也是可以定义一些filter的，因为这里没有用到就不介绍了。</p><p>QianmiStandardTokenizer继承自<code>org.apache.lucene.analysis.Tokenizer</code>类，该类需要重写Tokenizer类的<code>public boolean incrementToken()</code>和<code>public void reset()</code>方法。除此之外，Tokenizer还需要用到一些attribute来保存分词的信息。</p><h4 id="attribute属性"><a href="#attribute属性" class="headerlink" title="attribute属性"></a>attribute属性</h4><p>attribute属性用于保存我们分词的一些结果信息，例如分词本身、分词的类型、分词的位置、分词的长度，等等。如下我们定义了三个属性</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分词的属性</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">CharTermAttribute</span> <span class="variable">termAtt</span> <span class="operator">=</span> addAttribute(CharTermAttribute.class);</span><br><span class="line"><span class="comment">// 分词在原始文本的位置信息</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">OffsetAttribute</span> <span class="variable">offsetAtt</span> <span class="operator">=</span> addAttribute(OffsetAttribute.class);</span><br><span class="line"><span class="comment">// 分词的类型信息，例如字母、文字等等</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">TypeAttribute</span> <span class="variable">typeAtt</span> <span class="operator">=</span> addAttribute(TypeAttribute.class);</span><br></pre></td></tr></table></figure><p>当我们得到分词之后，只需要把这些信息保存到attribute中，后续的流程可以从这些attribute取出相应的数据。</p><h4 id="incrementToken-方法"><a href="#incrementToken-方法" class="headerlink" title="incrementToken()方法"></a>incrementToken()方法</h4><p>每一次该方法执行就会得到一个分词结果，如果该方法返回true则表示还存在分词可以继续获取，返回false则表示分词已经获取完毕，我们只需要在该方法中把得到的分词信息保存我们上面所说的attribute中，Lucene会从属性中获取到这些分词的结果信息。</p><p>需要注意的是，在执行这个方法时需要先执行<code>clearAttributes()</code>方法来清除attribute属性的中信息，目的是为了防止上一次的分词信息对这一次的分词结果产生影响。下面我们看一个该方法的例子</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">incrementToken</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    clearAttributes();</span><br><span class="line">    <span class="type">char</span>[] chars = <span class="keyword">new</span> <span class="title class_">char</span>[<span class="number">1</span>];</span><br><span class="line">    <span class="comment">// 原始文本数据可以通过Tokenizer类的input变量读取到</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> input.read(chars);</span><br><span class="line">    <span class="keyword">if</span> (count == -<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果原始数据读取完毕，结束分词</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 保存分词信息</span></span><br><span class="line">    termAtt.append(chars[<span class="number">0</span>]);</span><br><span class="line">    typeAtt.setType(<span class="string">&quot;default&quot;</span>);</span><br><span class="line">    <span class="comment">// 进入下一次分词</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="reset-方法"><a href="#reset-方法" class="headerlink" title="reset()方法"></a>reset()方法</h4><p>该方法在每段文本分词前都会调用，目的是恢复一些环境属性，防止多个文本的分词互相影响。注意在重写该方法时需要调用<code>super.reset()</code>以协助恢复一些Lucene本身的环境属性。假设我们需要在每次文本分析前恢复文本读取的offset变量</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reset</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="built_in">super</span>.reset(); <span class="comment">// 必须调用父类的reset方法</span></span><br><span class="line">    offset = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Lucene分词插件这一小节我们知道了如何定义Analyzer、Tokenizer以及如何在Tokenizer中实现分词逻辑。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><h3 id="一"><a href="#一" class="headerlink" title="(一)"></a>(一)</h3><p>除AnalysisPlugin之外，如果你想实现一些其它类型的插件，那么只需要创建一个继承自<code>Plugin</code>的类并且实现指定的插件接口即可，Elasticsearch所有的Plugin Interface在类<code>Plugin</code>的文档中都有<a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch/blob/v7.5.2/server/src/main/java/org/elasticsearch/plugins/Plugin.java#L55-L71">详细的介绍</a>。总结起来在Elasticsearch中创建插件的简单流程就是创建一个继承自<code>Plugin</code>类并且实现了指定类型插件（例如AnalysisPlugin）的接口的类。</p><h3 id="二"><a href="#二" class="headerlink" title="(二)"></a>(二)</h3><p>这里的<code>QianmiStandardAnalyzerProvider::new</code>是一个lambda表达式，其转化过程如下：</p><ol><li>这里需要一个实现了接口AnalysisModule.AnalysisProvider的类的对象；</li><li>AnalysisModule.AnalysisProvider接口有一个get方法为虚拟方法，实现该接口的类需要实现该方法；</li><li>我们并不需要真的去实现一个类并且让该类实现这个get方法，而是可以使用匿名内部类的方式实现；<ol start="0"><li>(如果不使用匿名内部类)</li><li>实现一个类，让该类实现AnalysisModule.AnalysisProvider接口；</li><li>该类也需要实现get方法，get方法的逻辑还是一样的；</li><li>之后在这里创建一个该类的对象即可；</li></ol></li><li>如果使用匿名内部类，则只需要实现get方法即可：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="title class_">AnalysisModule</span>.AnalysisProvider&lt;AnalyzerProvider&lt;? <span class="keyword">extends</span> <span class="title class_">Analyzer</span>&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> AnalyzerProvider&lt;? <span class="keyword">extends</span> <span class="title class_">Analyzer</span>&gt; get(IndexSettings indexSettings, Environment environment, String name, Settings settings) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">QianmiSubAnalyzerProvider</span>(indexSettings, environment, name, settings);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>由于该接口只需要实现一个get方法，所以可以使用lambda表达式对如上的代码优化如下：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(indexSettings, env, name, settings) -&gt; &#123;</span><br><span class="line">    <span class="keyword">return</span>  <span class="keyword">new</span> <span class="title class_">QianmiSubAnalyzerProvider</span>(indexSettings, env, name, settings);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>由于该方法的方法体只有一行，所以可以把上面的表达式进一步的优化为如下的lambda表达式：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(indexSettings, env, name, settings) -&gt; <span class="keyword">new</span> <span class="title class_">QianmiSubAnalyzerProvider</span>(indexSettings, env, name, settings)</span><br></pre></td></tr></table></figure></li><li>由于get方法的参数和后面创建对象的参数一致，所以可以使用lambda表达式进行进一步的优化：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QianmiSubAnalyzerProvider::<span class="keyword">new</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="三"><a href="#三" class="headerlink" title="(三)"></a>(三)</h3><p>如何在Lucene中调用Analyzer进行分词呢？在Lucene的<a target="_blank" rel="noopener" href="https://lucene.apache.org/core/8_3_0/core/org/apache/lucene/analysis/TokenStream.html">官网中有如下文档</a></p><p><img src="/images/20200428/1.png"></p><p>具体的使用方式如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">content</span> <span class="operator">=</span> <span class="string">&quot;如何在Lucene中调用Analyzer进行分词呢？在Lucene的官网中有如下文档&quot;</span>;</span><br><span class="line"><span class="type">Analyzer</span> <span class="variable">analyzer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">QianmiStandardAnalyzer</span>();</span><br><span class="line"><span class="type">TokenStream</span> <span class="variable">ts</span> <span class="operator">=</span> analyzer.tokenStream(<span class="string">&quot;&quot;</span>, content);</span><br><span class="line">ts.reset();</span><br><span class="line"><span class="type">CharTermAttribute</span> <span class="variable">termAtt</span> <span class="operator">=</span> ts.getAttribute(CharTermAttribute.class);</span><br><span class="line"><span class="keyword">while</span> (ts.incrementToken()) &#123;</span><br><span class="line">    System.out.println(termAtt.toString());</span><br><span class="line">&#125;</span><br><span class="line">ts.end();</span><br><span class="line">ts.close();</span><br></pre></td></tr></table></figure><h3 id="四"><a href="#四" class="headerlink" title="(四)"></a>(四)</h3><p>如何在Elasticseach中使用指定的分词器呢？在插件安装完毕之后，我们重启Elasticsearch节点。之后在创建索引时我们可以指定字段的类型</p><pre><code>PUT http://localhost:9200/test

&#123;
    &quot;settings&quot;: &#123;
        &quot;analysis&quot;: &#123;
            &quot;analyzer&quot;: &#123;
                &quot;default&quot;: &#123;
                    &quot;type&quot;: &quot;qm_standard&quot;
                &#125;,
                &quot;prefix&quot;: &#123;
                    &quot;type&quot;: &quot;sub&quot;,
                    &quot;section&quot;: &quot;0:3;0:5&quot;
                &#125;,
                &quot;postfix&quot;: &#123;
                    &quot;type&quot;: &quot;sub&quot;,
                    &quot;section&quot;: &quot;-6:-1;-4:-1&quot;
                &#125;
            &#125;
        &#125;
    &#125;,
    &quot;mappings&quot;: &#123;
        &quot;properties&quot;: &#123;
            &quot;title&quot;: &#123;
                &quot;type&quot;: &quot;text&quot;,
                &quot;analyzer&quot;: &quot;qm_standard&quot;
            &#125;
        &#125;
    &#125;
&#125;
</code></pre><p>如上我们可以使用默认的分析器如qm_standard，也可以根据已存在的分析器定义一些新的分析器，例如prefix分析器就来自于sub分析器。其中自定义分析器的配置信息会传到上面提到的AnalyzerProvider类的Settings变量中，我们可以使用这些配置信息来定义一些新的分词逻辑。</p><p>下面是一个使用指定分析器在Elasticsearch中进行分词测试的例子</p><pre><code>POST http://localhost:9200/test/_analyze

&#123;
    &quot;analyzer&quot;: &quot;postfix&quot;,
    &quot;text&quot;: &quot;TCC20040112442814525679&quot;
&#125;

&#123;
    &quot;tokens&quot;: [
        &#123;
            &quot;token&quot;: &quot;525679&quot;,
            &quot;start_offset&quot;: 17,
            &quot;end_offset&quot;: 22,
            &quot;type&quot;: &quot;word&quot;,
            &quot;position&quot;: 0
        &#125;,
        &#123;
            &quot;token&quot;: &quot;5679&quot;,
            &quot;start_offset&quot;: 19,
            &quot;end_offset&quot;: 22,
            &quot;type&quot;: &quot;word&quot;,
            &quot;position&quot;: 1
        &#125;
    ]
&#125;
</code></pre><h3 id="相关文档"><a href="#相关文档" class="headerlink" title="相关文档"></a>相关文档</h3><p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/reference/7.5/analysis.html">Elasticsearch的文本分析</a><br><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.5/index.html">ELasticsearch的插件体系</a><br><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.5/plugin-authors.html">使用Java实现Elasticsearch自定义插件的参考文档</a><br><a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch/tree/master/plugins/examples">Elasticsearch源码中关于插件部分的示例</a><br><a target="_blank" rel="noopener" href="https://kevinjiang.info/2017/10/01/%E8%87%AA%E5%AE%9A%E4%B9%89Lucene%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8Analyzer/">自定义Lucene的分词器Analyzer</a><br><a target="_blank" rel="noopener" href="https://lucene.apache.org/core/8_3_0/core/org/apache/lucene/analysis/package-summary.html#package.description">Package org.apache.lucene.analysis</a><br><a target="_blank" rel="noopener" href="https://lucene.apache.org/core/8_3_0/index.html">Apache LuceneTM 8.3.0 Documentation</a><br><a target="_blank" rel="noopener" href="https://github.com/RitterHou/elasticsearch-analysis">https://github.com/RitterHou/elasticsearch-analysis</a></p></div><div class="post-copyright"><div><strong>本文链接：</strong> <span title="聊聊Elasticsearch中的文本分析">https://www.nosuchfield.com/2020/04/28/Talk-about-text-analysis-in-Elasticsearch/</span></div><div><strong>版权声明： </strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处！</div></div><style>summary{cursor:pointer;margin-bottom:10px}summary:focus{outline:0}</style><script src="/js/code-enhancer.js"></script><script src="/js/pangu.min.js"></script><script>pangu.spacingPage()</script><script>function backToTop(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script defer src="https://cloud.umami.is/script.js" data-website-id="267e4aaf-8cb5-464d-b16c-89e66283e505"></script><div class="post-footer"><ul class="post-tag-list" itemprop="keywords"><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/ElasticSearch/" rel="tag">ElasticSearch</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E5%88%86%E8%AF%8D/" rel="tag">分词</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag">自然语言处理</a></li></ul><span onclick="backToTop()" class="top">返回顶部</span></div></article><footer><span>&copy; 2015 - 2025</span> <span class="author">Raymond</span> <span style="float:right"><span class="upyun">本网站由<a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"> <img src="/images/others/upyun.png"></a>提供CDN加速&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> <a class="filing" href="https://beian.miit.gov.cn/" target="_blank">苏ICP备15057335号</a> <a class="github" href="https://github.com/RitterHou" target="_blank">GitHub</a></span></footer></div></body></html>