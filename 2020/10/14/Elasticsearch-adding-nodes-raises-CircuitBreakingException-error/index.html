<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><title>Elasticsearch新增节点引发CircuitBreakingException错误 - 侯锐的思考与分享</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="keywords" content="Java,ElasticSearch,GC"><meta name="description" content="&lt;p&gt;那是一个平静的上午，我因为业务需要决定给我们的Elasticsearch（以下简称ES）集群添加几个节点，当前集群ES的版本为7.5.2，而待添加的节点都是从之前的1.5.2版本的ES集群中移出来的。当前的7.5.2集群已经有了6个数据节点，我这次的工作就是准备再添加5个新的数据节点上去。&lt;/p&gt;
&lt;p&gt;因为这5个节点都是之前1.5.2版本的ES集群的数据节点，并且在之前的集群中稳定的运行了好几年也没有出现过任何问题，所以我对它们的配置都是很放心的，这种大意的态度就为接下来的悲剧埋下了伏笔。因为这些节点在之前的集群中都没什么问题，所以我就一次性把这5个节点全部添加到了ES7集群中成为了数据节点。&lt;/p&gt;
&lt;h3 id=&#34;出现错误&#34;&gt;&lt;a href=&#34;#出现错误&#34; class=&#34;headerlink&#34; title=&#34;出现错误&#34;&gt;&lt;/a&gt;出现错误&lt;/h3&gt;
&lt;style&gt;
    pre {
        white-space: pre-wrap;
    }
&lt;/style&gt;


&lt;p&gt;节点添加之后，ES集群开始进行分片的重新平衡，整个集群开始进行分片的搬迁和复制操作，我们的工"><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="侯锐的思考与分享" type="application/atom+xml"></head><body><div class="container"><header class="header"><div class="blog-title"><a href="/" class="logo">侯锐的思考与分享</a><div class="subtitle"></div></div><nav class="navbar"><ul class="menu"><li class="menu-item"><a href="/" class="menu-item-link" data-no-instant>主页</a></li><li class="menu-item"><a href="/atom.xml" class="menu-item-link" data-no-instant>订阅</a></li><li class="menu-item"><a href="/search" class="menu-item-link" data-no-instant>搜索</a></li></ul></nav></header><article class="post"><div class="post-title"><h1 class="article-title">Elasticsearch新增节点引发CircuitBreakingException错误</h1></div><div class="post-meta"><span class="post-time">2020-10-14</span></div><div class="post-content"><p>那是一个平静的上午，我因为业务需要决定给我们的Elasticsearch（以下简称ES）集群添加几个节点，当前集群ES的版本为7.5.2，而待添加的节点都是从之前的1.5.2版本的ES集群中移出来的。当前的7.5.2集群已经有了6个数据节点，我这次的工作就是准备再添加5个新的数据节点上去。</p><p>因为这5个节点都是之前1.5.2版本的ES集群的数据节点，并且在之前的集群中稳定的运行了好几年也没有出现过任何问题，所以我对它们的配置都是很放心的，这种大意的态度就为接下来的悲剧埋下了伏笔。因为这些节点在之前的集群中都没什么问题，所以我就一次性把这5个节点全部添加到了ES7集群中成为了数据节点。</p><h3 id="出现错误"><a href="#出现错误" class="headerlink" title="出现错误"></a>出现错误</h3><style>pre{white-space:pre-wrap}</style><p>节点添加之后，ES集群开始进行分片的重新平衡，整个集群开始进行分片的搬迁和复制操作，我们的工作似乎很快就要完成了。很快，这种平静就被手机短信的铃声所打破了，短信提示业务突然开始出现了大量的读写失败错误！同时企业微信的告警群也开始大量告警，告警信息显示的都是<code>CircuitBreakingException</code>类型的异常，具体的报错信息我摘录部分如下</p><pre><code>[parent] Data too large, data for [&lt;transport_request&gt;] would be [15634611356/14.5gb], which is larger than the limit of [15300820992/14.2gb], real usage: [15634605168/14.5gb], new bytes reserved: [6188/6kb], usages [request=0/0b, fielddata=0/0b, in_flight_requests=6188/6kb, accounting=18747688/17.8mb]
</code></pre><p>其实报错的原因非常简单，就是当前内存已经触发了parent级别的circuit breaker，导致transport_request无法继续进行。因为如果继续进行transport_request则可能导致ES产生OutOfMemory错误，ES为了避免OOM会设置一些<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/reference/7.5/circuit-breaker.html">circuit breaker (断路器)</a>，这些断路器的作用就是在内存不够的时候主动拒绝接下来的操作，而不是进一步的分配内存最终产生OutOfMemoryError，断路器的作用就是保护整个进程不至于挂掉。</p><p>我们已经知道了报错的原因是Java进程的堆内存不够了，那么到底是什么原因导致了内存会不够呢？此时此刻我暂时没有心思考虑这些问题，新增加的5个节点都在频繁报内存不够的问题，这导致了大量的线上读写失败，我当前的首要目标就是解决这些报错。这就是我之前所说的伏笔了，因为我一次性把5个节点全部加到了集群中去，所以此时某个索引的某个分片的主分片和所有副本分片可能全部分布在这些我新添加的节点上，因而我不能一次性把这些节点全部停掉，因为这样会导致这些分片的数据彻底丢失从而使得整个集群变成红色。</p><p>事实上我当时因为告警太多太紧张了所以干了这种事情，即停止了多个节点，集群状态立即变成了红色。好在这些分片还存在于停止节点的磁盘上，集群变红之后我赶紧又把这些节点起了起来，集群才又脱离红色状态。之后我只能一边忍受着告警，一边默默地等待分片的复制，只有确认一个节点上不存在某一个分片的唯一分片数据之后，我才能把这个节点停掉。</p><p>Anyway，我一边忍受着告警一边停止节点，经过一段时间之后总算是把这5个节点都停掉了，内存不足导致的读写告警终于停止了。坑爹的是此时整个集群出现了一些unassigned的分片，即这些分片未能成功分配。我们使用如下命令找到所有还没有分配的分片(unassigned shards)，并且解释这些分片没能分配的原因</p><pre><code>GET /_cluster/allocation/explain
</code></pre><p>得到错误原因如下</p><pre><code>shard has exceeded the maximum number of retries
</code></pre><p>也就是说之前的内存错误导致了这些分片的分配失败，并且多次失败达到了最大的重试次数，此时ES放弃了对这些分片的分配操作。这种情况下我们只需要执行如下命令来手动开始进行重新分配分片</p><pre><code>POST /_cluster/reroute?retry_failed=true
</code></pre><p>之后集群会开始对这些未分配的分片进行分配，等待一段时间的分片分配和复制之后，整个集群终于重新恢复绿色了。</p><h3 id="出错原因"><a href="#出错原因" class="headerlink" title="出错原因"></a>出错原因</h3><p>首先我们想到的就是因为GC的问题导致内存没能及时的回收掉，剩余内存不够导致了错误。我们观察了G1垃圾收集器的GC日志，G1的日志大致分为如下三个部分</p><pre><code># 正常的YoungGC
Pause Young (Normal) (G1 Evacuation Pause)

# 伴随着YoungGC会有多次标记操作
Pause Young (Concurrent Start) (G1 Humongous Allocation)
Concurrent Cycle

# MixedGC
Pause Young (Prepare Mixed) (G1 Evacuation Pause)
Pause Young (Mixed) (G1 Evacuation Pause)
</code></pre><p>在观察了GC日志之后我们发现堆内存每每在已经达到了很高的占用率之后才会触发GC，这种情况就很有可能导致内存无法及时回收以及剩余的内存不足。如果我们能让GC更早的发生，那么就能够降低剩余内存不够的概率（虽然这样会因为GC的更加频繁而降低整个系统的吞吐量）。通过搜索我们在ES源码的<a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch/pull/46169/commits/f4b587257ffd9c7f3d2eecc7096a100b72cb46d6">Pull requests</a>中发现了如下的GC配置</p><pre><code>-XX:G1ReservePercent=25
-XX:InitiatingHeapOccupancyPercent=30
</code></pre><p><a target="_blank" rel="noopener" href="https://www.oracle.com/technical-resources/articles/java/g1gc.html">G1垃圾收集器的官方文档</a>中对这两个参数的解释如下</p><pre><code>-XX:G1ReservePercent=10
Sets the percentage of reserve memory to keep free so as to reduce the risk of to-space overflows. The default is 10 percent. When you increase or decrease the percentage, make sure to adjust the total Java heap by the same amount. This setting is not available in Java HotSpot VM, build 23.

-XX:InitiatingHeapOccupancyPercent=45
Sets the Java heap occupancy threshold that triggers a marking cycle. The default occupancy is 45 percent of the entire Java heap.
</code></pre><p>简单来说<code>-XX:G1ReservePercent</code>是保留的内存空间百分比，其目的是避免内存不够而导致的错误，默认值是10，我们将其提升到了25。<code>-XX:InitiatingHeapOccupancyPercent</code>是触发一次marking cycle的内存占用阈值百分比，默认是45，我们将其减小到了30。修改了这两个虚拟机参数之后，虚拟机就能够更早的进行GC，这样就会大大降低内存不够错误出现的概率。</p><p>修改完参数重新启动节点，这次我们一台一台的起，启动一台之后等待几个小时确认没有问题之后再启动下一台。此外，我们在新增节点之前先把<code>cluster.routing.allocation.enable</code>参数设置为<code>none</code>，等待节点确认启动完毕之后再把其设置为<code>all</code>，这样就可以手动控制分片分配的开始和停止。改完参数之后，节点已经不会频繁的发生内存不够的错误了，可见修改配置使得GC时间提前确实降低了GC过慢导致的内存不足问题。虽然一般的情况下节点内存已经不存在压力，但是此时还有另一个问题，就是在加入节点后搬迁分片时还是有几率触发节点内存不够的错误，此时我们只需要减慢分片搬迁的速度即可。</p><p>我们将<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/reference/7.5/shards-allocation.html">cluster.routing.allocation.node_concurrent_recoveries</a>的值从默认的2修改为1，这样可以降低同一时刻节点上搬迁的分片的数量。此外，我们通过设置<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/reference/7.5/recovery.html#_peer_recovery_settings">indices.recovery.max_bytes_per_sec</a>将每个节点分片搬迁速度从40mb&#x2F;s降低为20mb&#x2F;s，这也能减少分片搬迁时节点的内存压力。改完了这些配置之后，节点就再也没有出现过内存不够的错误了。</p><p>其实综上我们可以总结出这次出现问题的原因如下</p><ol><li>gc速度过慢</li><li>内存增长过快</li></ol><p>解决办法就相应地如下</p><ol><li>减小GC触发阈值，提升GC频率</li><li>减少数据同步速度，降低内存增加速度</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/qcloud1001/p/13451127.html">PB级大规模Elasticsearch集群运维与调优实践</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1493177">JVM调优实战：G1中的to-space exhausted问题</a></p><p><em>另外多扯一句，ES分片复制的时候，对于从开始复制到复制结束这段时间产生的数据，是由target的translog负责记录的，之后target会对复制来的数据和自己的translog数据进行合并，得到最终数据。target的数据来源如下：</em></p><table><thead><tr><th>复制开始前</th><th>复制过程中</th><th>复制结束后</th></tr></thead><tbody><tr><td>复制source分片的数据</td><td>复制过程中写入的本地translog数据</td><td>普通的分片本地写入数据</td></tr></tbody></table></div><div class="post-copyright"><div><strong>本文链接：</strong> <span title="Elasticsearch新增节点引发CircuitBreakingException错误">https://www.nosuchfield.com/2020/10/14/Elasticsearch-adding-nodes-raises-CircuitBreakingException-error/</span></div><div><strong>版权声明： </strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处！</div></div><style>summary{cursor:pointer;margin-bottom:10px}summary:focus{outline:0}</style><script src="/js/code-enhancer.js"></script><script src="/js/pangu.min.js"></script><script>pangu.spacingPage()</script><script>function backToTop(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script defer src="https://cloud.umami.is/script.js" data-website-id="267e4aaf-8cb5-464d-b16c-89e66283e505"></script><div class="post-footer"><ul class="post-tag-list" itemprop="keywords"><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/ElasticSearch/" rel="tag">ElasticSearch</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/GC/" rel="tag">GC</a></li><li class="post-tag-list-item"><a class="post-tag-list-link" href="/tags/Java/" rel="tag">Java</a></li></ul><span onclick="backToTop()" class="top">返回顶部</span></div></article><footer><span>&copy; 2015 - 2025</span> <span class="author">Raymond</span> <span style="float:right"><span class="upyun">本网站由<a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"> <img src="/images/others/upyun.png"></a>提供CDN加速&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> <a class="filing" href="https://beian.miit.gov.cn/" target="_blank">苏ICP备15057335号</a> <a class="github" href="https://github.com/RitterHou" target="_blank">GitHub</a></span></footer></div></body></html>